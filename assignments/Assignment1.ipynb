{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5182fd3b-fa90-4a6d-b146-4a5c631368eb",
   "metadata": {},
   "source": [
    "# Formalia\n",
    "\n",
    "Please read the [assignment overview page](https://laura.alessandretti.com/comsocsci2026/wiki_pages/Assignments.html) carefully before proceeding. The page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "__If you fail to follow these simple instructions, it will negatively impact your grade!__\n",
    "\n",
    "**Due date and time**: The assignment is due on Mar 3rd at 23:59. Hand in your Jupyter notebook file (with extension `.ipynb`) via DTU Learn _(Assignment 1)_. \n",
    "\n",
    "Remember to include in the first cell of your notebook:\n",
    "* the link to your group's Git repository \n",
    "* group members' contributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17914c-f125-4f70-b1a5-0c56641a0ea0",
   "metadata": {},
   "source": [
    "## Part 1: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25059f1b-25c3-47eb-8278-903bd7608576",
   "metadata": {},
   "source": [
    "> **Exercise: Ready made data vs Custom made data** In this exercise, I want to make sure you have understood they key points of my lecture and the reading. \n",
    ">\n",
    "> 1. What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book __(answer in max 150 words)__.\n",
    "> 2. How do you think these differences can influence the interpretation of the results in each study? __(answer in max 150 words)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c7606-9542-4ecd-9f6e-1c5e298883d6",
   "metadata": {},
   "source": [
    "## Part 2: Find Researchers using the OpenAlex API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed489a-1f99-4124-af1b-97f332fb0fe1",
   "metadata": {},
   "source": [
    "> **Exercise 3: Find potential Computational Social Scientists** In this exercise, we'll use the OpenAlex API to compile a list of researchers in the field of Computational Social Science, focusing on those who have attended the IC2S2 conference in 2025. This will not only later on help you understand the landscape of Computational Social Science research but also develop practical skills in data collection and analysis. \n",
    ">\n",
    "> Please read the text of the whole exercise before starting to work on it. \n",
    ">\n",
    "> **Steps**\n",
    "> \n",
    "> 1. **Retreive data.** Consider the set of unique researcher names that you collected in Week 1, Exercise 3. Use the _authors_ endpoint of the [OpenAlex API](https://docs.openalex.org/api-entities/authors) to _search_ these researchers in the database based on their names. Loop through the list and, for each researcher in your list, find: \n",
    ">     - their _id_: The OpenAlex ID for this author.\n",
    ">     - their _display\\_name_: The name of the author as a single string.\n",
    ">     - their _works\\_api\\_url_: A URL that will get you a list of all this author's works.\n",
    ">     - their _h\\_index_ : The h-index for this author.\n",
    ">     - their _works\\_count_: The number of  Works this this author has created.\n",
    ">     - their _country\\_code_: The country code of their last known institution\n",
    "> 2. **Data Storage** Store this information in a Pandas DataFrame and save it to file.\n",
    ">\n",
    ">    \n",
    "> **Handling Challenges**\n",
    "> \n",
    "> I expect that, while working on the steps above, you will encounter several obstacles. As you complete this exercise, you are expected to:     \n",
    ">\n",
    ">    - Identify problems that arise.      \n",
    ">    - Improve your solutions to address such problems, making reasonable decisions when data is incomplete or ambiguous.       \n",
    ">\n",
    "> **Reflection Questions**\n",
    ">  Answer the following questions __(max 150 words for each question)__: \n",
    ">\n",
    ">    - Which challenges did you encounter? How did you address them?\n",
    ">    - Choose one problem you faced while collecting the data and describe your solution. Why did you choose this approach, and what impact might it have on your data? \n",
    ">      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067acae",
   "metadata": {},
   "source": [
    "## Part 3: Collect Research Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ff170",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "> **Exercise 1: Collecting Research Articles from IC2S2 Authors**\n",
    ">\n",
    ">In this exercise, we'll leverage the OpenAlex API to gather information on research articles authored by participants of the IC2S2 2025 conference, referred to as *IC2S2 authors*. **Before you start, please ensure you read through the entire exercise.**\n",
    ">\n",
    "> \n",
    "> **Steps:**\n",
    ">  \n",
    "> 1. **Retrieve Data:** Start with the dataset of *IC2S2 authors* you collected in Week 2, Exercise 3 (called dataset D1 in the figure above). Use the OpenAlex API [works endpoint](https://docs.openalex.org/api-entities/works) to fetch their research articles. For each article, retrieve the following details:\n",
    ">    - _id_: The unique OpenAlex ID for the work.\n",
    ">    - _publication_year_: The year the work was published.\n",
    ">    - _cited_by_count_: The number of times the work has been cited by other works.\n",
    ">    - _author_ids_: The OpenAlex IDs for the authors of the work.\n",
    ">    - _title_: The title of the work.\n",
    ">    - _abstract_inverted_index_: The abstract of the work, formatted as an inverted index.\n",
    "> \n",
    ">     **Important Note on Paging:** By default, the OpenAlex API limits responses to 25 works per request. For more efficient data retrieval, I suggest to adjust this limit to 200 works per request. Even with this adjustment, you will need to implement pagination to access all available works for a given query. This ensures you can systematically retrieve the complete set of works beyond the initial 200. Find guidance on implementing pagination [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/paging#cursor-paging).\n",
    ">\n",
    "> 2. **Data Storage:** Organize the retrieved information into two Pandas DataFrames and save them to two files in a suitable format:\n",
    ">    - Dataset D2: The *IC2S2 papers* dataset should include: *id, publication\\_year, cited\\_by\\_count, author\\_ids*.\n",
    ">    - Dataset D3: The *IC2S2 abstracts* dataset should include: *id, title, abstract\\_inverted\\_index*.\n",
    ">  \n",
    ">\n",
    "> **Filters:**\n",
    "> To ensure the data we collect is relevant and manageable, apply the following filters:\n",
    ">     \n",
    ">    - Only include *IC2S2 authors* with a total work count between 5 and 5,000.    \n",
    ">    - Retrieve only works that have received more than 10 citations.    \n",
    ">    - Limit to works authored by fewer than 10 individuals.    \n",
    ">    - Include only works relevant to Computational Social Science (focusing on: Sociology OR Psychology OR Economics OR Political Science) AND intersecting with a quantitative discipline (Mathematics OR Physics OR Computer Science), as defined by their [Concepts](https://docs.openalex.org/api-entities/works/work-object#concepts). *Note*: here we only consider Concepts at *level=0* (the most coarse definition of concepts).     \n",
    ">\n",
    "> **Efficiency Tips:**\n",
    "> Writing efficient code in this exercise is **crucial**. To speed up your process:\n",
    "> \n",
    "> - **Apply filters directly in your request:** When possible, use the [filter parameter](https://docs.openalex.org/api-entities/works/filter-works) of the *works* endpoint to apply the filters above directly in your API request, ensuring only relevant data is returned. Learn about combining multiple filters [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists).  \n",
    "> - **Bulk requests:** Instead of sending one request for each author, you can use the [filter parameter](https://docs.openalex.org/api-entities/works/filter-works) to query works by multiple authors in a single request. *Note: My testing suggests that can only include up to 25 authors per request.*\n",
    "> - **Use multiprocessing:** Implement multiprocessing to handle multiple requests simultaneously. I highly recommmend [Joblibâ€™s Parallel](https://joblib.readthedocs.io/en/stable/) function for that, and [tqdm](https://tqdm.github.io/) can help monitor progress of your jobs. Remember to stay within [the rate limit](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication) of 100 requests per second.\n",
    ">\n",
    ">\n",
    ">   \n",
    "> For reference, employing these strategies allowed me to fetch the data in about 30 seconds using 5 cores on my laptop. I obtained a dataset of approximately 25 MB (including both the *IC2S2 abstracts* and *IC2S2 papers* files).\n",
    "> \n",
    ">\n",
    "> **Data Overview and Reflection questions:** Answer the following questions __(max 150 words for each question)__: \n",
    "> \n",
    "> - **Dataset summary.** How many works are listed in your Dataset D2 (*IC2S2 papers*) dataframe? How many unique researchers have co-authored these works?     \n",
    "> - **Efficiency in code.** Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?    \n",
    "> - **Filtering Criteria and Dataset Relevance** Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
