[
  {
    "objectID": "wiki_pages/SurveyWeek2.html",
    "href": "wiki_pages/SurveyWeek2.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\nAPIs\n\n\n\n\n\n\n\n\nAPI\nrate limits\nlink\n\n\n\n\nX API: Trends; returns trending topics for a specific geographic location, identified by a Yahoo. DM: Receive and send DMs (groups and one-one) Posts: Receive posts of a set user Users: retrieve profile information\nThe free version has a rate limit of 17 requests per 24 hours per user and per app\nhttps://docs.x.com/x-api/introduction\n\n\nWith the ORCID Public API, one can search the public ORCID registry and retrieve public information from individual ORCID records. Info like researcher metadata which can be name and biography, employments/affiliations, education, funding, works/publications, peer review, and other contributions (basically all that the user has set as public).\nAnonymous API - Rate limit: 12 requests/second - Burst limit: 40 requests/second - Usage quota: 25,000 reads per day - Quota scope: Per IP address Public (Registered) API - Rate limit: 12 requests/second - Burst limit: 40 requests/second - Usage quota: 100,000 reads per day - Quota scope: Per Client ID\nhttps://info.orcid.org/what-is-orcid/services/public-api/\n\n\nX (twitter) API: Trends; returns trending topics for a specific geographic location, identified by a Yahoo. DM: Receive and send DMs (groups and one-one) Posts: Receive posts of a set user Users: retrieve profile information\nThe free version has a rate limit of 17 requests per 24 hours per user and per app\nhttps://docs.x.com/x-api/introduction\n\n\nReal-time flight status and global aviation data.\n100 requests per month\nhttps://aviationstack.com/\n\n\nEverything is a graph, where nodes are users, photos etc. and edges are connections between them.\nThere are many different rate limits depending on what you are trying to do (Instagram, Facebook, Messenger)\nhttps://developers.facebook.com/docs/graph-api\n\n\n“The College Scorecard API offers access to a wealth of data about U.S. higher education institutions. This API enables developers to query and retrieve information about schools, programs, costs, graduation rates, admissions, and much more. This documentation will guide you through the process of accessing and utilizing the College Scorecard API.”\n1,000 requests/IP-address/hour with possibility of increase by contacting the provider per e-mail. Exceeding limit will return “429 Too Many Requests”.\nhttps://collegescorecard.ed.gov/data/api/\n\n\nThere are three major search services provided by dblp: one for publications, one for persons (authors/editors), and one for venues (journals/conferences/etc).\nUnkown - not mentionen in documentation\nhttps://dblp.org/faq/13501473.html https://dblp.org/search/publ/api for publication queries https://dblp.org/search/author/api for author queries https://dblp.org/search/venue/api for venue queries\n\n\nMetadata in JSON format describing the global research ecosystem. You can gather data on Works, Authors, Institutions, and Topics, all linked through a massive citation graph.\nThe free tier allows for up to 100,000 requests per day per user.\nThe official documentation and entry point for the API can be found at: docs.openalex.org\n\n\nOpenSky API: Allows one to pull different kinds of aerospace data. Example data pulls: Arrivals and departures from a specific airport, airplane trajectory, amount of flights in a given interval and much more\nI can pull data every ten seconds\nhttps://openskynetwork.github.io/opensky-api/python.html#return-types\n\n\nyou can read and write public content on reddit in structured JSON format. This includes posts, comments with full nested thread structure, subreddit metadata, some user profile information.\na bit unclear but should be around 1000/10 min\nhttps://developers.reddit.com/docs/capabilities/server/reddit-api\n\n\nI have gathered data about the national brutto product and the average happines of the danish citizens. The api provided table data in the json format, which easily can be plottet.\nI could only find a limit of the amount of datacells a request could pull which was 1.000.000 cells.\nhttps://www.dst.dk/da/Statistik/hjaelp-til-statistikbanken/api\n\n\nThe API has access to 45 different databases and nearly 16000 time series indicators that can get data about the health, welath and culture of many countries around the world.\nNo limits are specified in the documentation\nhttps://datahelpdesk.worldbank.org/knowledgebase/articles/889392#how-to-use-the-v2-indicators-api\n\n\nInformation about videos about Computational Social Science on Youtube and whick kinds of users watch them\nThe YouTube Data API have a default quota allocation of 10,000 units per day. Search costs 100 units.\nhttps://developers.google.com/youtube/v3/getting-started\n\n\nThis “Indicators” API has access to 45 different databases and nearly 16000 time series indicators that give information about the health, wealth, and culture of many countries around the world.\nThe website itself does not specify any rate limits (we’ve checked the documentation), but a quick Google search shows a limit of 60 indicators at once for the Indicators API.\nhttps://datahelpdesk.worldbank.org/knowledgebase/articles/889392#how-to-use-the-v2-indicators-api\n\n\nAir Quality Index Data\nmax 1000 requests per minute\nhttps://aqicn.org/api/\n\n\nThe API we have chosen is the FBI Crime Data API, because It is a list of crimes, where “it includes information about each offense, such as the time of day an incident occurred, the demographics of the offenders/victims, the known relationships between the offenders and victims, and many other details around how and where crime occurs.”\nWe have found that the rate limit of calling the FBI crime data API to be 10 request per window.\napi.usa.gov/crime/fbi/cde/\n\n\nThe OpenAlex API provides information about scientific papers and researchers, including authors, institutions, publication year, and citation counts. This can be used to analyse who works in Computational Social Science and how the field develops over time.\n100,000 credits per day with a free API key. Maximum 100 requests per second for all users.\nhttps://docs.openalex.org/",
    "crumbs": [
      "Extra content",
      "Survey Week 2"
    ]
  },
  {
    "objectID": "wiki_pages/Home.html",
    "href": "wiki_pages/Home.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "wiki_pages/Home.html#intro",
    "href": "wiki_pages/Home.html#intro",
    "title": "",
    "section": "Intro",
    "text": "Intro\nWelcome to the wiki for the course Computational Social Science (02467) offered by the Technical University of Denmark. This is the main page, where you can access the weekly exercises. If you take a look in the side-bar, you can read about the administrative details (including the course overview), assignments, books, and more.\nThe class is taught flipped classroom style, where the the lecture and homework elements of a course are reversed. You’ll be able to view short video lectures during the class session, so in-class time can be devoted to exercises, projects, or discussions. Check out the Before Week 1 lecture to learn more."
  },
  {
    "objectID": "wiki_pages/Home.html#lectures",
    "href": "wiki_pages/Home.html#lectures",
    "title": "",
    "section": "Lectures",
    "text": "Lectures\nBefore week 1. Take a look at this page before you do anything. This class most likely works a little bit differently from other classes you’ve taken. The notebook explains pretty much everything - the rest will be explained during the lectures. In case the link doesn’t work, you can also see the file here on Github, but the videos won’t display properly\nWeek 1: Intro to Computational Social Science. This week is all about getting started: learning about how the course works, make sure you master the tools you need to follow the class. I will give you an introduction to the field of Computational Social Science. And, we will start with something hands-on: you will learn about web-scraping and start gathering some data from the web.\n\nReading: Bit by Bit, chapter 1 Start by reading the Introduction of the book, where you will get you an understanding of the history of the field and the general framework.\nReading: Bit by Bit, chapter 6 Read the Ethics chapter of the book. Here, I don’t expect you to read all the details. However, I want to make sure you get an overall understanding of the ethical challenges and some of the approaches that are used in the field to deal with these complex issues. You can focus on sections 6.4 and 6.6."
  },
  {
    "objectID": "wiki_pages/Evaluation.html",
    "href": "wiki_pages/Evaluation.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Wiki",
      "Evaluation"
    ]
  },
  {
    "objectID": "wiki_pages/Evaluation.html#how-your-work-is-evaluated",
    "href": "wiki_pages/Evaluation.html#how-your-work-is-evaluated",
    "title": "",
    "section": "How Your Work Is Evaluated",
    "text": "How Your Work Is Evaluated\nThe final grade is an evaluation of your work in the entire course.\n\n50% comes from Assignments 1 and 2\n\n50% comes from the Final Project (Assignments A and B)",
    "crumbs": [
      "Wiki",
      "Evaluation"
    ]
  },
  {
    "objectID": "wiki_pages/Evaluation.html#feedback-scale",
    "href": "wiki_pages/Evaluation.html#feedback-scale",
    "title": "",
    "section": "Feedback Scale",
    "text": "Feedback Scale\nThe grade comes together from the numerical evaluation of each sub-assignment. The meaning of the numbers is:\n\n\n\n\n\n\n\nScore\nMeaning\n\n\n\n\n0\nInsufficient: minimum requirements not met\n\n\n1\nSufficient: meets minimum course criteria\n\n\n2\nGood: solid work with clear understanding and correct application\n\n\n3\nExcellent: high-quality work demonstrating depth, rigor, and clarity\n\n\n\nThese numbers are intended as feedback. There is no trivial mapping between the standard danish grading scale and the feedback provided as part of the class.",
    "crumbs": [
      "Wiki",
      "Evaluation"
    ]
  },
  {
    "objectID": "wiki_pages/Evaluation.html#evaluation-criteria",
    "href": "wiki_pages/Evaluation.html#evaluation-criteria",
    "title": "",
    "section": "Evaluation Criteria",
    "text": "Evaluation Criteria\n\nAssignments 1 & 2\nGrades are based on:\n\nCorrectness and completeness of each exercise\nQuality of the notebook (clear structure, well-documented code, clear explanations of results)\n\n\n\nFinal Project (Assignments A & B)\nThe project grade reflects both technical quality and communication quality.\n\n1. Project design\nWe look at whether the project makes sense as a whole. - Is the motivation clear and meaningful? - Is the dataset suitable for the question being asked? - Do the methods and conclusions actually answer the research question?\n\n\n2. Explainer notebook\nYour notebook should make it easy to understand what you did and why. - Is data collection and preprocessing clearly documented? - Are the methods explained in a way others can follow? - Is there real interpretation and reflection, not just outputs? - Is the notebook well structured and readable?\n\n\n3. Use of Network Science\nWe evaluate how well you use network science tools - Are the chosen techniques appropriate for the question? - Are they applied correctly? - Is the analysis thorough, rather than just a few basic measures?\n\n\n4. Use of text analysis methods\nSame idea for the text analysis. - Are the methods suitable for the problem? - Are they implemented correctly? - Is the analysis deep enough to support your conclusions?\n\n\n5. Website\nYour project should be understandable to someone outside the course. - Are the visualizations informative and well chosen? - Are explanations clear and accessible to a broad audience? - Is the website easy to navigate and pleasant to use?\n\n\n6. What makes a project excellent:\nThese additional aspects can lift a project from solid to outstanding: - Creativity and original thinking\n- Taking on a challenging or ambitious question/dataset - Showing awareness of ethical issues (data use, bias, privacy, limitations)",
    "crumbs": [
      "Wiki",
      "Evaluation"
    ]
  },
  {
    "objectID": "wiki_pages/Assignments.html",
    "href": "wiki_pages/Assignments.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\nTimeline\nAssignment 1.\nPosted: During Lecture 4\nDue: Tuesday, Mar 3rd, 23:59\nAssignment 2.\nPosted: During Lecture 8\nDue: Tuesday, Apr 7th, 23:59\nProject Assignment A\nSlides Due: Tuesday, Apr 21st, 23:59\nPresentation: Wednesday, Apr 22nd (in class)\nProject Assignment B\nDue: Friday, May 15th, 23:59\n\n\nAssignments 1 and 2\nThe lectures in this class run over 8 weeks. Each week, we will post a number of exercises. After a set of lectures, I will post an assignment. The assignment is a subset of the exercises. This means that, if you solve the exercises each week, the assignments will be easy.\n\nFormalia regarding Assignments 1 & 2\n\nCreate a repository on GitHub or GitLab, where we can assess the commit history of the individual members of the group. The repository should stay private until the assignment is due.\nCollaboratively work on the assignment as a group.\nThe repo should include a Jupyter notebook (named Assignment1.ipynb or Assignment2.ipynb) that includes the solution to all exercises in the assignment.\nImportant: The first cell of the notebook should include the link to your Github repository and a contribution statement.\nFor the delivery:\n\nmake your repository public (on the day of the assignment submission)\n\nupload the Jupyter notebook on DTU Learn\n\nmake sure that your code runs and renders all images, prints, etc. We recommend restarting the kernel under ‘Kernel’ and then clicking Cell –&gt; Run all before uploading.\n\ndouble check that your file renders correctly. Remember that you’ll be annoyed to get bad evaluations because no-one could see your plots.\n\nTo help me navigate the Notebook, it’s a good idea to repeat the question you’re answering.\nTry to control the length of your notebook. While grading, I look at how you prioritise material and express yourself clearly and succinctly.\nRead the text carefully - make sure you understand the question. And make sure that you answer all sub-questions, etc. (It’s easy to miss something, so be thorough).\nDo not solve all exercises in a single code cell. Split your code according to the questions\nThe notebook is designed to contain your code, so do include it. But do keep it short & neat (minimize long outputs, etc)\nFormat your plots properly. Axes must be labeled, make sure there’s text explaining the figure, etc.\nMake sure that you use references when they’re needed and follow academic standards.\nBe precise, write in objective language (avoid: “I think …”, “In my opinion…”, etc) - if you make an observation, support it with data.\n\n\n\n\nProject Assignment\nThe point of the Project Assignments is to try out the skills you’ve learned in the course on your own dataset. We’ve been working on understanding networks and natural language processing, so the idea is to find a dataset to analyze that will let you show off what you can do.\n\nStandford Large Networks Dataset Collection is a collection of large network datasets.\nWikipedia API You can collect Wikipedia data for whatever you are into.\nYou can also use data from specialized wikis (examples include Wookiepedia, Game of Thrones Wiki, Simpson’s Wiki, etc)\nNetzschleuder. Another collection of Network Datasets.\nSocial Networks Awesome datasets. Another collection of public social network datasets.\n\nAnd new ideas for datasets are very welcome. You should work with something that interests you - that way the project will be much more fun to work on. You will be working together in groups just as for the first two assignments.\n\nProject Assignment A\nThe first part of the final project is a 5 minute presentation, which should explain the central idea/concept that you will investigate in your final project. You’re making the presentation so that I can give you feedback, and you can get ideas from other groups. The presentation must contain the following:\n\nAn explanation of the central idea behind your project (what is the idea?, why is it interesting? which datasets did you need to explore the idea?, how did you download them)\nAn outline on the elements you’ll need to get to your goal & the implementation plan.\nA walk-through of your preliminary data-analysis, addressing\n\nWhat is the total size of your data? (MB, number of rows, number of variables, etc)\nWhat is the network you will be analyzing? (number of nodes? number of links?, degree distributions, what are node attributes?, etc.)\nWhat is the text you will be analyzing?\nHow will you tie the two together?\n\n\nBut other than that, there are no constraints on the format. Note that you will present to the entire class.\nHanding in the assignment: Simply upload your slides on DTU Learn. Note that since Project Assignment A now requires significant data-work, you have 2 weeks to create the video presentation.\n\n\nProject Assignment B\nThe deliverables for the Final project will be\nA website. The website should contain your analysis, it should tell the story about the data that you’re interested in getting across. The website should not be technical, but rather aim at using visualization and explanation to get your insights across to a non-scientific reader.\nAn explainer Notebook. The Notebook should contain all the behind the scenes stuff, details on the dataset, why you’ve selected this particular dataset, explanations of your choices regarding network analysis, etc. You should link to the notebook from the site. The idea is that you can create much more complex, fun and interactive analysis (and visualizations) on line. So the website is a way for you to present your work in a way that everyone can understand it … including dynamic visualizations, interactive analysis, etc, etc … that would not work on a piece of paper.\n\n\nMore about the website\nThis part of the assignment is quite free. The main point of the website is to present your idea/analyses to the world in a way that showcases your use of what you’ve learned in class. It can be as simple as an old fashioned static web-page, and as complicated as you want it to be. Let your creativity run wild (but keep in mind that this is not a coding class - we care mostly about content and analysis).\nThe website should be self-contained and tell the story of your dataset without the need for the Explainer Notebook (the purpose of the notebook is to provide additional details for interested readers). Here are some requirements\n\nThe page should say clearly what the dataset is and give the reader some idea of its most important properties (kind of Project Assignment A-style).\nThe page should contain your network and text analysis (that’s the main part).\nThere should be download options for data sets (so the user can play around).\nYou must link to the Explainer Notebook (more details below) that explains the details of your analysis (including all of the machine learning, the model selection, etc). You can achieve this with a link to a notebook displaying on the nbviewer.\nFor hosting, I recommend using your DTU website or Github pages.\n\n\n\nMore on the explainer notebook\nThe notebook should contain your analysis and code. Please structure it into the following 4 sections\n\nMotivation\n\nWhat is your dataset?\nWhy did you choose this/these particular dataset(s)?\nWhat was your goal for the end user’s experience?\n\nBasic stats. Let’s understand the dataset better\n\nWrite about your choices in data cleaning and preprocessing\nWrite a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)\n\nTools, theory and analysis. Describe the process of theory to insight\n\nTalk about how you’ve worked with text, including regular expressions, unicode, etc.\nDescribe which network science tools and data analysis strategies you’ve used, how those network science measures work, and why the tools you’ve chosen are right for the problem you’re solving.\nHow did you use the tools to understand your dataset?\n\nDiscussion. Think critically about your creation\n\nWhat went well?\nWhat is still missing? What could be improved? Why?",
    "crumbs": [
      "Wiki",
      "Assignments"
    ]
  },
  {
    "objectID": "lectures/Week2.html",
    "href": "lectures/Week2.html",
    "title": "Week 2",
    "section": "",
    "text": "In this lecture, we will continue to work on the data. As you may have already noticed from the previous class, working with real-world data is not easy! Today, we will learn more about different data sources for Computational Social Science, and some of the challenges that they present.\nHere is the plan for the lecture.\n\nPart 1: We will learn the differences between different kinds of data sources. We will go through the theory, then I will ask you to reflect about what you have learned through an exercise.\nPart 2: In the second part of this class, I will introduce you to APIs. We will then use one API to continue our investigation of the field of Computational Social Science in a data-driven way.",
    "crumbs": [
      "Lectures",
      "Week 2"
    ]
  },
  {
    "objectID": "lectures/Week2.html#today",
    "href": "lectures/Week2.html#today",
    "title": "Week 2",
    "section": "",
    "text": "In this lecture, we will continue to work on the data. As you may have already noticed from the previous class, working with real-world data is not easy! Today, we will learn more about different data sources for Computational Social Science, and some of the challenges that they present.\nHere is the plan for the lecture.\n\nPart 1: We will learn the differences between different kinds of data sources. We will go through the theory, then I will ask you to reflect about what you have learned through an exercise.\nPart 2: In the second part of this class, I will introduce you to APIs. We will then use one API to continue our investigation of the field of Computational Social Science in a data-driven way.",
    "crumbs": [
      "Lectures",
      "Week 2"
    ]
  },
  {
    "objectID": "lectures/Week2.html#part-1-data-sources-for-computational-social-science",
    "href": "lectures/Week2.html#part-1-data-sources-for-computational-social-science",
    "title": "Week 2",
    "section": "Part 1: Data Sources for Computational Social Science",
    "text": "Part 1: Data Sources for Computational Social Science\nWe have seen how DATA is central to Computational Social Science. But what data sources are we talking about? What are the limitations of different types of data sources? In the video below, I will give you an introduction to different types of data sources. As an example, I will introduce you to two studies that use two very different datasets to answer related questions.\n\nVideo lecture: Watch the video below about Data Sources in Computational Social Science.\nOptional Reading: The Spread of Behavior in an Online Social Network Experiment. This is the article describing the first study I talk about in the video.\nOptional Reading: Exercise contagion in a global social network. This is the article describing the second study I talk about in the video.\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"Hr5yKJaQUhE\",width=600, height=337.5)\n\n\n        \n        \n\n\nIn this course, we are going to focus mostly on observational data collected online to address social science questions. So, I would like us to reflect a little bit more on what it means to use Ready made data in the social science, and understand its advantages and challenges. This is something that you can read about in Sections 2.1 to 2.3 of the book Bit by Bit.\n\nReading: Bit by Bit, sections 2.1 to 2.3 Read sections 2.1 to 2.3. I don’t expect you to read all the details, but to have a general understanding of advantages and challenges of large observational datasets (a.k.a. Big Data/Ready made data) for social science research. If you have problems accessing the book, you can find a pdf version of the book here.\n\n\nExercise 1: Ready made data vs Custom made data In this exercise, I want to make sure you have understood they key points of my lecture and the reading. Remember to come and ask me, if you have any question about this!\n\nWhat are pros and cons of the custom-made data used in Centola’s experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides’s study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book.\n\nHow do you think these differences can influence the interpretation of the results in each study?",
    "crumbs": [
      "Lectures",
      "Week 2"
    ]
  },
  {
    "objectID": "lectures/Week2.html#part-2-using-apis-to-download-data",
    "href": "lectures/Week2.html#part-2-using-apis-to-download-data",
    "title": "Week 2",
    "section": "Part 2: Using APIs to download data",
    "text": "Part 2: Using APIs to download data\nIn this class, we will work with Ready made data. The second thing we will learn today is how to get data ready made data using APIs. We will do it using the Academic Graph API provided by Semantic Scholar. The Academic Graph API enables you to gather information on scientists and their publications.\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"7AQO3vJptvg\",width=600, height=337.5)\n\n\n        \n        \n\n\n\nExercise 2: APIs for Computational Social Science research. In this excercise, I ask you to look for an API that one could use for Computational Social Science. Starting from your answers, I will compile a list of useful APIs, and share it with the class next week. It may come handy when you start working on your project.\nNote: the answers to the surveys on DTU Learn are not contributing to your final grade, but it is still important to fill them in, because they help me ensure that you are on track with the material..\n\nUse the web to look for one API that could be used for gathering interesting data (from a Computational Social Science perspective).\n\nData description: describe in a couple of lines the data types that you can gather using this API.\n\nRate limits: What are the rate limits of the| free version of the API?\n\nLink to the API: Add the link to the API.\n\n\nGo on DTU Learn and fill in the survey Week 2 - APIs.",
    "crumbs": [
      "Lectures",
      "Week 2"
    ]
  },
  {
    "objectID": "lectures/Week2.html#prelude-to-part-3-pandas-dataframes",
    "href": "lectures/Week2.html#prelude-to-part-3-pandas-dataframes",
    "title": "Week 2",
    "section": "Prelude to part 3: Pandas Dataframes",
    "text": "Prelude to part 3: Pandas Dataframes\nBefore starting, we will learn a bit about pandas dataframes, a very user-friendly data structure that you can use to manipulate tabular data. Pandas dataframes are built using numpy, which is in turn built in C, so they are a quite efficient data structure. You will find it quite useful :)\nPandas dataframes should be intuitive to use. I suggest you to go through the 10 minutes to Pandas tutorial to learn what you need to follow the rest of the course.\nVideo lecture: Watch the video below about Pandas, here is the notebook I used in the video\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"pM7IKcyfOV4\",width=600, height=337.5)",
    "crumbs": [
      "Lectures",
      "Week 2"
    ]
  },
  {
    "objectID": "lectures/Week2.html#part-3-getting-data-on-computational-social-scientists-from-the-openalex-api",
    "href": "lectures/Week2.html#part-3-getting-data-on-computational-social-scientists-from-the-openalex-api",
    "title": "Week 2",
    "section": "Part 3: Getting data on Computational Social Scientists from the OpenAlex API",
    "text": "Part 3: Getting data on Computational Social Scientists from the OpenAlex API\nAll right, let’s dive in! We’re going to start collecting data on Computational Social Scientists and their work. As you will see, getting to the final dataset will take a few iterations and will require you to handle lots of data, but I’m confident you’ll find it rewarding in the end. To help you throughout, I’ve broken down the process into manageable steps and provided strategies to help you tackle each one effectively.\nFeel free to team up with a classmate or two for this project (remember, you’ll be submitting assignments as a group of three). If you run into any issues or have questions, don’t hesitate to ask me. Working with real-world data can be frustrating at times. I’m here to help!\n\nExercise : Find potential Computational Social Scientists In this exercise, we’ll use the OpenAlex API to compile a list of researchers in the field of Computational Social Science, focusing on those who have attended the IC2S2 conference in 2025. This will not only later on help you understand the landscape of Computational Social Science research but also develop practical skills in data collection and analysis.\nPlease read the text of the whole exercise before starting to work on it.\nSteps\n\nRetreive data. Consider the set of unique researcher names that you collected in Week 1, Exercise 3. Use the authors endpoint of the OpenAlex API to search these researchers in the database based on their names. Loop through the list and, for each researcher in your list, find:\n\ntheir id: The OpenAlex ID for this author.\ntheir display_name: The name of the author as a single string.\ntheir works_api_url: A URL that will get you a list of all this author’s works.\ntheir h_index : The h-index for this author.\ntheir works_count: The number of Works this this author has created.\ntheir country_code: The country code of their last known institution\n\nData Storage Store this information in a Pandas DataFrame and save it to file.\n\nHandling Challenges\nI expect that, while working on the steps above, you will encounter several obstacles. As you complete this exercise, you are expected to:\n\nIdentify problems that arise.\n\nImprove your solutions to address such problems, making reasonable decisions when data is incomplete or ambiguous.\n\nReflection Questions Answer the following questions:\n\nWhich challenges did you encounter? How did you address them?\nChoose one problem you faced while collecting the data and describe your solution. Why did you choose this approach, and what impact might it have on your data?\n\nRemember, if you’re unsure about any steps or encounter problems, please reach out.\n\n\n\nStart by making a single API request, inspect the response carefully, and try a few different names to discover potential issues and address them.\nOnce you’re satisfied with your approach, run it for all authors. If you like, after your code works, you can ask an LLM for suggestions on how to improve efficiency.",
    "crumbs": [
      "Lectures",
      "Week 2"
    ]
  },
  {
    "objectID": "lectures/Before_week_1.html",
    "href": "lectures/Before_week_1.html",
    "title": "How to take this class",
    "section": "",
    "text": "Welcome to the Computational Social Science course. This class probably works a little differently from other classes you’ve taken. To avoid any confusion, I’ve created this notebook to explain how everything works.\nEach week of this class is a Jupyter notebook like this one. In order to successfully follow the class, you simply start reading from the top, following the instructions. As you read, you will encounter three things that should cause you to take break from reading or sometimes leave the page for a little bit:\n\nInstructions to go check out a video lecture.\nInstructions to read something (I don’t use a single textbook, so will make sure to link to the relevant text).\nInstructions to solve a few exercises. Exercises should be solved within the Jupyter notebook. The exercises are the most important part of the class. First, it is through the exercises that you will learn the course material, and, secondly, the exercises lay the foundations for the two mandatory course assignment (due after week 4 and week 8).\n\nIn this first notebook, we just get started with some important info: - Part 1: Introductions. Here you get to know me, your teacher.\n- Part 2: How we do things. Here, you learn a bit about the philosophy behind this course.\n- Part 3: Tools. We talk about the tools we will use in this course.",
    "crumbs": [
      "Lectures",
      "How to take this class"
    ]
  },
  {
    "objectID": "lectures/Before_week_1.html#overview",
    "href": "lectures/Before_week_1.html#overview",
    "title": "How to take this class",
    "section": "",
    "text": "Welcome to the Computational Social Science course. This class probably works a little differently from other classes you’ve taken. To avoid any confusion, I’ve created this notebook to explain how everything works.\nEach week of this class is a Jupyter notebook like this one. In order to successfully follow the class, you simply start reading from the top, following the instructions. As you read, you will encounter three things that should cause you to take break from reading or sometimes leave the page for a little bit:\n\nInstructions to go check out a video lecture.\nInstructions to read something (I don’t use a single textbook, so will make sure to link to the relevant text).\nInstructions to solve a few exercises. Exercises should be solved within the Jupyter notebook. The exercises are the most important part of the class. First, it is through the exercises that you will learn the course material, and, secondly, the exercises lay the foundations for the two mandatory course assignment (due after week 4 and week 8).\n\nIn this first notebook, we just get started with some important info: - Part 1: Introductions. Here you get to know me, your teacher.\n- Part 2: How we do things. Here, you learn a bit about the philosophy behind this course.\n- Part 3: Tools. We talk about the tools we will use in this course.",
    "crumbs": [
      "Lectures",
      "How to take this class"
    ]
  },
  {
    "objectID": "lectures/Before_week_1.html#part-1-introductions",
    "href": "lectures/Before_week_1.html#part-1-introductions",
    "title": "How to take this class",
    "section": "Part 1: Introductions",
    "text": "Part 1: Introductions\nLet’s start with some introductions. In the video below you will find out a little bit about me, the path that led me to study computational social science and what my research is about.\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"J6_f-LDLgyY\",width=800, height=450)\n\n\n        \n        \n\n\nOptional reading material\nIf you are curious about Complex Systems Science (one of the topics I discussed in the video above), you can come and ask me and/or have a look at these websites:\nComplexity Explained A webpage offering a very concise explanation of the key concepts in complex systems science. As a bonus, the explanations come with powerful interactive visualizations. The website is developed by prominent researchers in Complex Systems science.\nComplexity Explorer This is an awesome platform developed by one of the most important institutions in the field. It offers courses, tutorials and lectures on complex systems science.\n\nYour turn.\nI would like to know a little bit about your expectations for this class and level of previous knowledge around some of the topics and technical aspects that we will work on throughout this class. Please go to DTU Learn and fill the self-assessment and expectations survey.\nThe survey is completely anonymous. It will just be a way for me to have an idea of who you are as a group.",
    "crumbs": [
      "Lectures",
      "How to take this class"
    ]
  },
  {
    "objectID": "lectures/Before_week_1.html#part-2-how-we-do-things",
    "href": "lectures/Before_week_1.html#part-2-how-we-do-things",
    "title": "How to take this class",
    "section": "Part 2: How we do things",
    "text": "Part 2: How we do things\n\nVideo lecture: Start by watching the “How we do things” video below to learn the pedagogical aspects of how the class works.\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"W5dqAukzjDc\",width=800, height=450)",
    "crumbs": [
      "Lectures",
      "How to take this class"
    ]
  },
  {
    "objectID": "lectures/Before_week_1.html#part-3-tools-we-use-in-this-class",
    "href": "lectures/Before_week_1.html#part-3-tools-we-use-in-this-class",
    "title": "How to take this class",
    "section": "Part 3: Tools we use in this class",
    "text": "Part 3: Tools we use in this class\n\n1) Github for collaborative development\nIn this course, we will use Github to develop code collaboratively.\nHow to use Github for developing your Project Assignment It is required that you and your group use Github to work on your assignemnts. In all group assignments, you will be asked to submit a link to a Git repository in which the commit history of the individual members of the group can be assessed. This is for me to check how the group has collaboratively developed code for the assignment (see more in the course Wiki).\nIf you are not familiar with Git/Github, I suggest going through these two tutorials:\n\nIntro Git Tutorial\nIntermediate Git Tutorial\n\n\n\n2) Teams for communication\nIn this course, we will use Teams to communicate. You can join the Teams Channel directly from DTU Learn.\nHow to use Teams:\n\nAsking questions publicly: If you have a question that may be useful to others, please post it on a public channel, so that everyone can see the reply.\nAsking questions privately: If you have a question that is irrelevant to others, you can send a private message to me.\nDiscuss among you: Feel free of course use Teams to discuss with your team members, or the rest of the class.\n\nIMPORTANT: You can expect to receive an answer from me within a few days.\n\n\n3) Python and Jupyter notebooks\nThe exercise you will solve during the class require using Python and Jupyter Notebooks. I suggest to use the Anaconda distribution with Python 3.10 or newer. Note that if you want to use another setup, that’s fine.\nIf you are not familiary with Jupyter notebooks, you can follow these instructions:\n\nDownload the Anaconda distribution of Python here.\nCreate an Anaconda environment for this class: conda create -n comsocsci2026\nActivate the environment “conda activate comsocsci2026”.\nInstall jupyter “conda install jupyter”.\nStart up the notebook by typing “jupyter notebook” and your terminal, and everything should be ready to use in your favorite browser.\n\nSuper important Everything we do going forward in this class will depend on you being comfortable with Python. There is simply no way that you will be able to do well, if you’re also struggling with Python on top of everything else you’ll be learning.\nSo if you’re not 100% comfortable with Python, I recommend you follow a tutorial to teach you Python, for example this one, before proceeding.\n\n\n4) Large Language Models\n\n\nWe all use LLMs for learning and coding. In this course, I’ll try to give you practical tips on how to use LLMs in a way that can actually help you learn.\n\n\nThroughout the class, you will see exercises that end with a purple note containing suggestions on how you might experiment with LLMs while working on that problem.\n\n\nUsing LLMs in teaching is still evolving, and I’m actively exploring ways we can use them to support your learning in this course. Let me start by addressing some of the questions you might naturally have.\n\n\nDo we need to code ourselves when AI can already write code?\n\n\nYes, because coding is not just writing syntax, but rather designing solutions to problems.\n\n\nThat often means:\n\n\n\nTaking a big problem\n\n\nBreaking it into smaller parts\n\n\nBreaking those parts down again\n\n\nSolving each piece\n\n\nPutting everything back together into a working solution\n\n\n\nAlong the way, you make many decisions. And taking good decisions require deep understanding of the problem and the methods. That is understanding that one can only get from experience.\n\n\nMy goal in this course is to help you build that experience, so you become a better problem solver, not just someone who can run code.\n\n\nBut can LLMs just do all the above for me?\n\n\nSometimes, but it depends on the problem.\n\n\nFor small, well-defined tasks, LLMs are often excellent. If the task is “sort this array,” they’ll almost certainly give you a correct solution.\n\n\nBut as problems get more complex and open-ended, LLMs become less reliable on their own. They can struggle with:\n\n\n\nDeciding how to structure the problem\n\n\nChoosing the right steps\n\n\nChecking whether the full solution actually makes sense\n\n\n\nAnd this is why, as of today, we still need a smart human in the loop. \n\n\nLet’s take one of our class projects: building a collaboration network of computational social scientists from publication data. This is a complex, multi-step problem. It requires making many design choices and solving several subproblems in the right order.\n\n\nThis is why when I tried to give this problem directly to state-of-the-art LLMs, the solutions I got have many flaws.\n\n\nSo… can I use LLMs to solve exercises?\n\n\nYou shall make sure you use them in a smart and thoughtful way\n\n\nFor the best learning and deepest understanding, the most effective approach is to first try solving the exercise on your own. Struggling a bit, making mistakes, and figuring things out yourself is a crucial part of learning. Once you have your own solution you can use an LLM to review it, improve it, or suggest alternative approaches.\n\n\nThat said, I know that sometimes you may be short on time or feel stuck. If you do rely more heavily on an LLM, keep in mind that many exercises are complex enough that if you paste the entire problem into an LLM, the answer will likely be incomplete or incorrect.\n\n\nFor many coding tasks, a better way to use LLMs is to:\n\n\n\nFirst break the problem into smaller steps yourself\n\n\nThen use an LLM to help with specific “simple enough” subproblems\n\n\nCarefully check whether the final solution works and makes sense by understanding the code and verifying the output\n\n\nIterate and improve your solution\n\n\n\nIf you work this way, LLMs can be a powerful learning tool. If you don’t, they can easily become a shortcut that prevents you from developing the skills this course is designed to build, and introduce mistakes into your work.",
    "crumbs": [
      "Lectures",
      "How to take this class"
    ]
  },
  {
    "objectID": "lectures/Week1.html",
    "href": "lectures/Week1.html",
    "title": "Week 1",
    "section": "",
    "text": "This first lecture will go over a few different topics to get you started:\n\nPart 1: You picked this course in Computational Social Science but… What does that even mean?? The first thing we will do today is to learn a bit more about it by reading some chapters of the book and listen to a short lecture by me.\nPart 2: In the second part of this class, I will introduce you to a topic we will work on for the rest of this course and I will ask you to reflect upon it.\nPart 3: In the final part of this class, we will start working on something hands on. We will use Web scraping to gather some data.",
    "crumbs": [
      "Lectures",
      "Week 1"
    ]
  },
  {
    "objectID": "lectures/Week1.html#overview",
    "href": "lectures/Week1.html#overview",
    "title": "Week 1",
    "section": "",
    "text": "This first lecture will go over a few different topics to get you started:\n\nPart 1: You picked this course in Computational Social Science but… What does that even mean?? The first thing we will do today is to learn a bit more about it by reading some chapters of the book and listen to a short lecture by me.\nPart 2: In the second part of this class, I will introduce you to a topic we will work on for the rest of this course and I will ask you to reflect upon it.\nPart 3: In the final part of this class, we will start working on something hands on. We will use Web scraping to gather some data.",
    "crumbs": [
      "Lectures",
      "Week 1"
    ]
  },
  {
    "objectID": "lectures/Week1.html#part-1-intro-to-computational-social-science",
    "href": "lectures/Week1.html#part-1-intro-to-computational-social-science",
    "title": "Week 1",
    "section": "Part 1: Intro to Computational Social Science",
    "text": "Part 1: Intro to Computational Social Science\nWhat is Computational Social Science? In the video below, I will give you some answers. There will be a little bit of history, example of topics and datasets, an overview of the methods, and some reflections on the challenges faced by researchers, including in relation to Ethics and Privacy.\n\nVideo lecture: Watch the video below about Computational Social Science\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"3dA1GYdSg-A\",width=600, height=337.5)\n\n\n        \n        \n\n\nIn this course, we are going to read some parts of the amazing book by Matthew Salganik “Bit By Bit: Social Research in the Digital Age”. Salganik is a professor in Sociology at Princeton, and an active researcher in Computational Social Science. You can read the book online, but I encourage you to buy it.\n\nReading from the Bit by Bit book.\nReading: Bit by Bit, chapter 1 Start by reading the Introduction of the book, where you will get you an understanding of the history of the field and the general framework.\nReading 2: Bit by Bit, chapter 6 Read the Ethics chapter of the book. Here, I don’t expect you to read all the details. However, I want to make sure you get an overall understanding of the ethical challenges and some of the approaches that are used in the field to deal with these complex issues. You can focus on sections 6.4 and 6.6.\n\n\nExercise 1: Design your own Computational Social Science project.\nYou now have a sense of how digital data can be used to study society. Work in pairs.\n\nStart from an interesting social science question. Come up with one question about human behavior or society that you find interesting. Examples:\n\nHow does misinformation spread online?\n\nHow does working from home reshape when and where people spend time?\nWhat makes certain neighborhoods feel safer than others?\n\nMake it “computational”. Briefly write:\n\nWhat kind of digital data could help study this? (e.g., social media data, news, reviews, …) Important: Your data source must be something you could realistically access as a student (e.g., public datasets, open APIs, or data you could collect yourself)\nWho/what are the units of analysis? (people, neighborhoods, communities, …)\nOne measurable variable you could extract from data to answer your question.\n\nEthics check.\n\nName one ethical risk and how you might reduce it. You can refer to the areas of difficulties described in chapter 6.6 of Salganik’s book.\n\nSubmit. On DTU Learn, submit your best idea through the Survey “Design your own Computational Social Science project.”:\n\nResearch question\n\nData source\n\nUnit of analysis\nOne measurable variable\n\nOne ethical concern.\n\n\nNOTE 1: Reach out during class/on Teams if you have doubts about the feasibility of your idea, or whether your research question makes sense.\nNOTE 2: The goal of this exercise is for you to start reasoning about data sources and topics. You are not yet deciding the project you will work on for your final assignment.\nNOTE 3: All group members should submit the survey answers. Identical answers are perfectly ok.\n\n\n\nFor this exercise, I strongly recommend that you first develop your research question and basic idea without using an LLM. Once you feel you have a solid research idea and a dataset, you can use an LLM to refine and improve your solution. Why not start with an LLM? Because:\n\n\n\nAt the time of writing this exercise, even when prompted carefully, state-of-the-art LLMs (I tested ChatGPT 5.2) can suggest ideas that sound impressive but are not feasible\n\n\nBy spending time srtuggling with problems yourself, you build deeper understanding (which means you get also better at spotting weaknesses/feasibility issues)",
    "crumbs": [
      "Lectures",
      "Week 1"
    ]
  },
  {
    "objectID": "lectures/Week1.html#part-2-a-topic-we-will-work-on-for-the-rest-of-this-course",
    "href": "lectures/Week1.html#part-2-a-topic-we-will-work-on-for-the-rest-of-this-course",
    "title": "Week 1",
    "section": "Part 2 : A topic we will work on for the rest of this course",
    "text": "Part 2 : A topic we will work on for the rest of this course\nAll right so, as I promised this course will be very hands-on. We will learn some of the methods and modelling approaches used in Computational Social Science and we will put this learning into practice. The way we will do it is that we will apply the methods we learn to study a specific topic throughout the rest of this course.\nIn this video, I will explain to you what the whole project will be about. Ready? Watch the video below!\n\nVideo lecture: Watch the video below\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"uEJltY5Pv1U\",width=600, height=337.5)\n\n\n        \n        \n\n\n\nExercise 2 : Understanding Computational Social Science in a data-driven way\nIn the rest of this course, we will do a meta-study on the field of Computational Social Science, by analyzing researchers, their collaborations, and their scientific production.\nWork in pairs. Think of one type of data you could collect to study the field (for example: publications, co-authorship networks, conference participation, social media presence, or research topics).\nFor this one dataset, briefly answer:\n\nWhat is the data source?\n\nWhat would this data help us understand about the field?\n\nHow would you practically collect it? (e.g., API, open dataset, scraping).\n\nWhat is one limitation or bias of this data?\n\nGo to DTU Learn and fill the Survey “Data for Computational Social Science”.\n\nRemember that in this exercise, there is not a single correct answer. There could be multiple ways to gather data, and different data sources could shed lights on different aspects of scientists’ works and interactions.\n\n\n\nAs in the previous exercise, I recommend starting with your own idea. It can be fun to then using an LLM to refine or stress-test it. For example, you could ask an LLM whether there are feasibility issues you may have missed.",
    "crumbs": [
      "Lectures",
      "Week 1"
    ]
  },
  {
    "objectID": "lectures/Week1.html#prelude-to-part-3-basic-html",
    "href": "lectures/Week1.html#prelude-to-part-3-basic-html",
    "title": "Week 1",
    "section": "Prelude to part 3: Basic HTML",
    "text": "Prelude to part 3: Basic HTML\nIn the final part of this class, we will talk about web-scraping. For web-scraping, you need a little bit of knowledge about the structure of web-pages. The standard way to write web-bages is to use a language called HTML.\nUseful tutorial: If you are not familiar with HTML, I recommend reading this tutorial.\nUseful resource: HTML pages are built in a hierarchical structure and are composed of elements such as tables, titles, paragraphs, sections, etc. A complete list of HTML elements can be found here.",
    "crumbs": [
      "Lectures",
      "Week 1"
    ]
  },
  {
    "objectID": "lectures/Week1.html#part-3-using-web-scraping-to-gather-data.",
    "href": "lectures/Week1.html#part-3-using-web-scraping-to-gather-data.",
    "title": "Week 1",
    "section": "Part 3: Using web-scraping to gather data.",
    "text": "Part 3: Using web-scraping to gather data.\nAll right, so now it’s really time to start working on something a bit hands-on. The first thing we need to do is indeed to get DATA. As I said a few times by now, in this class we will do things from scratch. One of the ways to gather data from the web is to use web-scraping, which basically means getting information directly from web-pages. In the video below, I will give a brief overview on how to web-scrape web pages.\n\nVideo lecture: Watch the video below about web scraping.\n\nHere, you can find the notebook that I use in the video.\n2026 update: The video below was recorded few years back. Since then, the Wikipedia page that I parse in the video has been updated, so you are not able to run the old notebook. Here, you can find an updated notebook that you can run..\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"nK_d0UQp4cE\",width=600, height=337.5)\n\n\n        \n        \n\n\n\nExercise 3 : Web-scraping the list of participants to the International Conference in Computational Social Science\nIt’s time to put things into practice. Remember that our goal will be to gather a dataset describing Computational Social Scientists and their work. As we have discussed, the field of Computational Social Science is loosely defined. To gather data, we will start from the list of researchers that have joined the most important scientific conference in Computational Social Science in 2025. The conference is called International Conference in Computational Social Science (IC2S2 in short). The assumption here is that the scientists who contribute to this conference are at the core of the field of Computational Social Science.\nYou can find the program of the 2025 edition of the conference at this link, and the list of organizers at this link. The conference program included many different contributions: keynote presentations, parallel talks, tutorials, posters.\n\nInspect the HTML of the two pages and use web-scraping to get the names of all researchers that contributed to the conference in 2025. The goal is the following:\n\nget as many names as possible including: organizers, keynote speakers, chairs, authors of parallel talks and authors of posters;\n\nensure that the collected names are complete and accuarate as reported in the website (e.g. both first name and family name);\n\nensure that no name is repeated multiple times with slightly different spelling.\n\nSome ideas you can apply for success:\n\nInspect the page through your web browser to identify the elements of the page that you want to collect. Ensure you understand the hierarchical structure of the page, and where the elements you are interested in are located within this structure.\nUse the BeautifulSoup Python package to navigate through the hierarchy and extract the elements you need from the page.\nYou can use the find_all method to find elements that match specific filters. Check the documentation of the library for detailed explanations on how to set filters.\nParse the strings to ensure that you retrieve “clean” author names (e.g. remove commas, or other unwanted charachters).\nIn some cases, the webpage only displays data that is actually loaded from an external file (for example a CSV or JSON file). If you notice this by inspecting the page, you may be able to download that file directly instead of scraping the HTML. This is much easier, cleaner, and less error-prone than parsing the rendered page.\n\nCreate the set of unique researchers that joined the conference and store it into a file.\n\nImportant: If you notice any issue with the list of names you have collected (e.g. duplicate/incorrect names), come up with a strategy to clean your list as much as possible.\n\nGo to DTU Learn and fill the Survey “Web Scraping”.\nHow many unique researchers do you get?\nExplain the process you followed to web-scrape the pages. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices.\n\n\n\n\nLLMs will likely not be able to solve this exercise correctly on their own (I tested GPT-5.2). The key is to break the problem into smaller subproblems by inspecting the HTML of the pages and identifying exactly which elements you need to extract. If you feel lazy, you can use an LLM to help write small, targeted functions for specific extraction steps. But make sure you carefully check the code and the output of each step. Iterate on your solution until the results are correct and complete.",
    "crumbs": [
      "Lectures",
      "Week 1"
    ]
  },
  {
    "objectID": "lectures/Week3.html",
    "href": "lectures/Week3.html",
    "title": "Week 3",
    "section": "",
    "text": "Today’s lecture has two main focuses:\nPart 1: Wrapping up Data Collection: We’re finishing our deep dive into data collection using the OpenAlex API. We’ll retrieve:\nThe goal is to become comfortable handling large, real-world datasets.\nA little warning: In this week’s exercises you will retreive and process a large amount of data. If you find the process tedious and overwhelming at times, you are not alone! That’s a completely normal part of dealing with real data.\nBut don’t worry, I’ll guide you through step by step, and I’m always available to help, on Teams or in class. There are three simple rules for success:\nParts 2 and 3: Introduction to Data Visualization + Visualizing Distributions: Next, we shift to a lighter topic: Data Visualization. There will be some video lectures on general aspects of Data Visualization. Then we’ll start with visualizing histograms. You may feel already confident with plotting histograms, but we’ll explore new angles, especially when it comes to visualizing data that’s spread out across a wide range. You will start seeing the value of the data we’ve collected and starting to analyze it in practical ways.\nThis is probably the most intense lecture of the course in terms of coding. Once you are on the other side from this one, it will feel much smoother from there.",
    "crumbs": [
      "Lectures",
      "Week 3"
    ]
  },
  {
    "objectID": "lectures/Week3.html#part-1-wrapping-up-the-data-collection",
    "href": "lectures/Week3.html#part-1-wrapping-up-the-data-collection",
    "title": "Week 3",
    "section": "Part 1: Wrapping up the data collection",
    "text": "Part 1: Wrapping up the data collection\nYou’re now equipped to tackle more challenging tasks with the OpenAlex API. In this part of the course, we will build the Computational Social Science dataset step by step.\nThe key idea is that each dataset builds on the previous one. Think of this as a pipeline:\n- Stage 1: We start from a list of IC2S2 authors (from Week 2).\n- Stage 2: We collect their scientific papers.\n- Stage 3: From those papers, we identify their co-authors — researchers who wrote papers together with them — and then collect papers written by those co-authors.\n- Stage 4: Finally, we merge everything into one unified dataset representing the Computational Social Science field.\nWhy do we collect data this way? Researchers do not attend all conferences, even if those conferences are relevant to their work. If we only included IC2S2 participants, we would capture only a visible subset of the field. For this reason, we expand our scope to include the collaborators of the IC2S2 authors — the IC2S2 co-authors. By moving one step outward through co-authorship links, we obtain a more realistic approximation of the Computational Social Science research community. In network science, this strategy is closely related to snowball sampling: starting from a seed set and expanding through observed connections.\nIn Exercise 1, you will collect all research articles written by the IC2S2 authors (~1500 researchers). Even though this seems manageable, efficiency already matters — the code you write here will be reused in the next exercise.\nIn Exercise 2, we expand to include the collaborators of the IC2S2 authors (the IC2S2 co-authors). This step significantly increases the dataset size. You’ll be managing many API requests here. This makes it crucial to write code that’s not just functional but also efficient.\nOverview of the Datasets we collect \n\n⚠ Before Exercise 1: Get an API key\nStarting February 13, 2026, an API key is required to use the OpenAlex API.\nSteps for success:\n\nSign up and authenticate.\n\nCopy your API key.\n\nadd api_key=YOUR_KEY to your API calls. When you use the requests library you can add it to your params dictionary.\n\n\n\nExercise 1: Collecting Research Articles from IC2S2 Authors\nIn this exercise, we’ll leverage the OpenAlex API to gather information on research articles authored by participants of the IC2S2 2025 conference, referred to as IC2S2 authors. Before you start, please ensure you read through the entire exercise.\nSteps:\n\nRetrieve Data: Start with the dataset of IC2S2 authors you collected in Week 2, Exercise 3 (called dataset D1 in the figure above). Use the OpenAlex API works endpoint to fetch their research articles. For each article, retrieve the following details:\n\nid: The unique OpenAlex ID for the work.\npublication_year: The year the work was published.\ncited_by_count: The number of times the work has been cited by other works.\nauthor_ids: The OpenAlex IDs for the authors of the work.\ntitle: The title of the work.\nabstract_inverted_index: The abstract of the work, formatted as an inverted index.\n\nImportant Note on Paging: By default, the OpenAlex API limits responses to 25 works per request. For more efficient data retrieval, I suggest to adjust this limit to 200 works per request. Even with this adjustment, you will need to implement pagination to access all available works for a given query. This ensures you can systematically retrieve the complete set of works beyond the initial 200. Find guidance on implementing pagination here.\nData Storage: Organize the retrieved information into two Pandas DataFrames and save them to two files in a suitable format:\n\nDataset D2: The IC2S2 papers dataset should include: id, publication_year, cited_by_count, author_ids.\nDataset D3: The IC2S2 abstracts dataset should include: id, title, abstract_inverted_index.\n\n\nFilters: To ensure the data we collect is relevant and manageable, apply the following filters:\n\nOnly include IC2S2 authors with a total work count between 5 and 5,000.\n\nRetrieve only works that have received more than 10 citations.\n\nLimit to works authored by fewer than 10 individuals.\n\nInclude only works relevant to Computational Social Science (focusing on: Sociology OR Psychology OR Economics OR Political Science) AND intersecting with a quantitative discipline (Mathematics OR Physics OR Computer Science), as defined by their Concepts. Note: here we only consider Concepts at level=0 (the most coarse definition of concepts).\n\nEfficiency Tips: Writing efficient code in this exercise is crucial. To speed up your process:\n\nApply filters directly in your request: When possible, use the filter parameter of the works endpoint to apply the filters above directly in your API request, ensuring only relevant data is returned. Learn about combining multiple filters here.\n\nBulk requests: Instead of sending one request for each author, you can use the filter parameter to query works by multiple authors in a single request. Note: My testing suggests that can only include up to 25 authors per request.\nUse multiprocessing: Implement multiprocessing to handle multiple requests simultaneously. I highly recommmend Joblib’s Parallel function for that, and tqdm can help monitor progress of your jobs. Remember to stay within the rate limit of 100 requests per second.\n\nFor reference, employing these strategies allowed me to fetch the data in about 30 seconds using 5 cores on my laptop. I obtained a dataset of approximately 25 MB (including both the IC2S2 abstracts and IC2S2 papers files).\nData Overview and Reflection questions: Answer the following questions:\n\nDataset summary. How many works are listed in your Dataset D2 (IC2S2 papers) dataframe? How many unique researchers have co-authored these works?\n\nEfficiency in code. Describe the strategies you implemented to make your code more efficient. How did your approach affect your code’s execution time?\n\nFiltering Criteria and Dataset Relevance Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?\n\n\n\n\nTo maximise learning and ensure a correct solution, avoid LLMs to solve the exercise. As ususal it is important to solve the exercise in a “modular” way. Start by making an API request for a single author, inspect the response carefully, and try a few different names to discover potential issues and address them.\nOnce you’re satisfied with your approach, work on scaling up the solution and run the requests for many authors. If you’d like, once you have a working code, you can ask an LLM for suggestions on how to improve efficiency.\n\n\n\nExercise 2: Collecting Data from IC2S2 Co-Authors In this exercise, we aim to collect detailed information on the co-authors of the IC2S2 authors and their publications. You’ll find that the code you developed previously can be efficiently reused here. Please ensure you read the entire exercise before beginning.\nSteps:\n\nRetreive information about IC2S2 co-authors. Start with the unique author IDs in the dataset D2 ( IC2S2 papers) you compiled in Exercise 1 above. Use the OpenAlex API authors endpoint for each author, to collect the following information:\n\ndisplay_name: The name of the author as a single string.\nworks_api_url: A URL that will get you a list of all this author’s works.\nh_index : The h-index for this author.\nworks_count: The number of Works this this author has created.\ncountry_code: The country code of their last known institution\n\nImportant: Exclude the IC2S2 authors from this query since you already have their data. Hint: Remember you can collect data in bulks.\nData Storage Compile the retrieved information into a Pandas DataFrame and save it. This is dataset D4 (IC2S2 co-authors). Choose an appropriate file format for storage.\nRetreive works from IC2S2 co-authors. Using the same procedure as in Exercise 1, gather data on the works by IC2S2 co-authors to create Dataset D5 (co-authors abstracts) and Dataset D6 (co-authors papers). Save the two dataframes to files. You can use exactly the same code you used in Exercise 1, with two small adjustments:\n\nExclude from this query the works by IC2S2 authors, since you already have them.\nWhen saving the co-authors papers dataframe: Include in the author_ids field only those authors who are either IC2S2 authors or IC2S2 co-authors. We just discard all other authors.\n\nCombine the data from authors and co-authors. We will now create our final dataframes and save them to file:\n\nThe CSS Authors dataset (D1+D4). Concatenate the IC2S2 authors (Dataset D1) dataset with the IC2S2 co-authors dataset (Dataset D4). There should not be duplicates here, but if there is any, remove them.\nThe CSS Papers dataset (D2+D5). Concatenate the IC2S2 papers (Dataset D2) with the co-authors papers dataset (Dataset D5), then drop duplicate works. Remove papers with only one author (we are interested in collaborative works here).\nThe abstracts dataset (D3+D6). Concatenate the IC2S2 abstracts dataset (Dataset D3) with the IC2S2 co-authors abstracts (Dataset D6), then drop duplicates.\n\nNote: You can delete the intermidiate files you have saved, we won’t use them from now on.\n\nFilters:\n\nApply the same filtering criteria as in Exercise 1 above to maintain consistency and relevance in the data collected.\n\nEfficiency Tips:\n\nImplement the efficiency enhancements suggested in Exercise 1. Here, they are even more crucial here due to the larger volume of data. For reference, employing these strategies allowed me to perform step 1 above in about 1:40 minutes and step 3 in 10:48 minutes using 5 cores on my laptop. I obtained a dataset of approximately 383 MB.\nHere are additional tips if you find that, even after optimizing, the code still takes long to run:\n\nTeamwork: Consider splitting the API requests among your team members.\n\nLeave the computer to it: Ensure your laptop is plugged into power and that sleep mode is disabled before you start running your code. Then, take a step back. You can stretch, grab a coffee, or get a snack, and just leave your computer fetch the data. Personally, I find it satisfying to let the machine work while I do something else. Coming back to find all tasks completed feels just great. And won’t make the whole process faster anyways :)\n\n\n\n\nExercise 3: Data Overview and Reflection questions: Answer the following questions:\n\nDataset summary. What is the final length of your “CSS Authors”, “CSS Papers” and “CSS Abstracts” datasets?\n\nEfficiency in code. Describe the strategies you implemented to make your code more efficient. How did your approach affect your code’s execution time?\n\nReflection. Consider the instruction (in step 3 above) to include only IC2S2 authors or IC2S2 co-authors in the author_ids field of the co-authors papers dataframe. Why do you think I asked you to apply this specific filtering criterion? Discuss how limiting the dataset to these authors affects the dataset’s relevance and integrity.\n\n\nGo on DTU Learn and fill in the survey Week 3 - The final dataset. This is a way for me to check/ensure that you as a class are up to speed with the material.",
    "crumbs": [
      "Lectures",
      "Week 3"
    ]
  },
  {
    "objectID": "lectures/Week3.html#part-2-intro-to-data-visualization",
    "href": "lectures/Week3.html#part-2-intro-to-data-visualization",
    "title": "Week 3",
    "section": "Part 2: Intro to Data Visualization",
    "text": "Part 2: Intro to Data Visualization\nGreat job getting all the data together. That was quite some effort.\nNow comes the most interesting part: exploring the data. This is an opportunity to understand the field of Computational Social Science using data to apply the methods we will develop throughout the course.\nBefore we begin the analysis, we’re going to talk about Data Visualization. In the two videos below, I will: (i) introduce general concepts on Data Visualization and (ii) present a few tips and techniques to improve the visual quality of your plots in Python.\nIn the assignments, I expect your plots to be informative, well-designed and clear to intepret. These aspects will be part of the evaluation.\n\n\nVideo Lecture: Intro to Data Visualization\n\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"oLSdlg3PUO0\",width=800, height=450)\n\n\n        \n        \n\n\nBefore even starting visualizing some cool data, I just want to give a few more practical tips for making good plots in matplotlib. Unless you feel like you are already a pro-visualizer, those should be pretty useful to make your plots look much nicer. Paying attention to details can make an incredible difference when we present our work to others.\nNote: there are many Python libraries to make visualizations. For this course, we will use matplotlig, which is one of the most widely used and flexible visualization libraries.\n\nVideo Lecture: How to improve your plots\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"sdszHGaP_ag\",width=800, height=450)",
    "crumbs": [
      "Lectures",
      "Week 3"
    ]
  },
  {
    "objectID": "lectures/Week3.html#part-3-visualizing-distributions",
    "href": "lectures/Week3.html#part-3-visualizing-distributions",
    "title": "Week 3",
    "section": "Part 3: Visualizing distributions",
    "text": "Part 3: Visualizing distributions\nRelying solely on summary statistics like the mean, median, and standard deviation to understand your dataset can sometimes be misleading. It’s very good practice, to begin your analysis by visualizing the data distribution. Observing the probability distribution of data points can reveal a wealth of insights.\nThe problem is that real-world datasets often cover a wide range of values, spanning several orders of magnitude. Hence, basic methods of plotting histograms may not effectively represent these datasets. However, there are techniques to address this challenge and enhance visualization.\nIn the video lecture below, I demonstrate how to plot histograms for datasets with significant heterogeneity. The techniques are shown using two examples: a financial dataset on stock prices and returns, and data on the number of comments posted by Reddit users. But these methods are universally applicable. You can use them to visualize any type of data.\n\nVideo Lecture: Plotting histograms and distributions\n\n\n\nYouTubeVideo(\"UpwEsguMtY4\",width=800, height=450)\n\n\n        \n        \n\n\n\nExercise 4: Analyzing Author Citations In this exercise, we aim to explore the distribution of citations per author within the field of Computational Social Science. Our objectives are twofold:\n\nLearn to Plot Distributions: We’ll tackle the challenge of visualizing distributions for heterogeneous data, a common scenario in Computational Social Science.\n\nInvestigate Author Recognition: We’ll analyze how recognition (measured in citations) varies for Computational Social Scientists from different countries.\n\nDataset: Use the “CSS Authors dataset” you prepared in Exercise 2 above.\nTasks: 1. Data Preparation:.\n\nExtract the total number of citations for each author from the dataset and store this information in an array.\n\n\nPlotting the Overall Citation Distribution:.\n\nUse numpy.histogram to create a histogram of citations per author. Consider the following when plotting your histogram:\n\nNumber of bins: The default behavior of numpy.histogram is to create 10 equally spaced bins. However, you should customize this to suit your data. Experiment with different numbers and sizes of bins to find the most informative visualization. Too few bins may oversimplify your data, while too many can result in a fragmented appearance.\n\nLinear vs. Logarithmic Binning Choose the approrpiate binning:\n\nUse logarithmic binning for heterogeneous data that has many extreme values (usuall in the right tail), creating bins with numpy.logspace.\n\nElse, use linear binning, creating bins with numpy.linspace.\n\n\nNormalization Where appropriate, you can convert your histogram into a Probability Density Function:\n\nset the density=True argument in numpy.histogram. This normalizes the histogram so the area under the curve equals 1, providing insights into the probability distribution of citations.\n\n\n\nComparative Histograms by Country:\n\nIdentify the top 5 countries by the number of authors. For each of these countries, plot the distribution of ciations per author (as a line plot). Overlay these histograms on the same figure for comparison.\n\nReflection questions:\n\nDid you choose linear or logarithmic binning for the histograms in tasks 2 and 3? Why?\n\nDid you normalize the histograms? why? Describe in your own words the difference between normalized and non-normalized histograms.\n\nHow does author recognitation, as captured by the total number of citations, varies among authors in the whole dataset and in the selected countries?\n\n\n\n\n\nHere, I strongly recommend you fully solve the exercise by yourself. Once you have a working solution, you can play with an LLM to improve the visualizations and make them look nicer.",
    "crumbs": [
      "Lectures",
      "Week 3"
    ]
  },
  {
    "objectID": "wiki_pages/Books.html",
    "href": "wiki_pages/Books.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nFor this course, we only use books that you can access for free on line. But I encourage you to get the books and support the authors - that’s how we’ll keep getting new, awesome textbooks. Books are listed in the order we use them.\n\nOn Computational Social Science\nBit by Bit, by Matthew J. Salganik\n\n\nOn Data Visualization\nScientific Visualization: Python + Matplotlib, by Nicolas P. Rougier (this is a technical book, we will use it very little, but I encourage you to check it out if you are into beautiful visualisations)\n\n\nOn Network Science\nNetwork Science, by Albert-Laszlo Barabasi.\n\n\nOn Natural Language Processing.\nNatural Language Processing with Python - Analyzing Text with the Natural Language Toolkit, by Steven Bird, Ewan Klein, and Edward Loper.",
    "crumbs": [
      "Wiki",
      "Books"
    ]
  },
  {
    "objectID": "wiki_pages/Groups.html",
    "href": "wiki_pages/Groups.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\nGroups\n\nAssignments are submitted in groups\n\nGroups should have 3 people\n\nYou should be in the same group for the whole semester\n\nPlease form your group within the first few weeks\n\nYou’ll sign up via DTU Learn\n\nEveryone in the group should understand all parts of the assignment. Each of you should all be able to explain and solve each exercise. If something is unclear, talk it through with your group or ask me, otherwise you’re missing part of the point of the course.\nYou can work in a group of 2, but all reports are graded the same way regardless of group size, so smaller groups should expect a bit more work.",
    "crumbs": [
      "Wiki",
      "Groups"
    ]
  },
  {
    "objectID": "wiki_pages/Schedule.html",
    "href": "wiki_pages/Schedule.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nActivity\nDetails\nMonth\nWeekday\n\n\n\n\nFeb 4\nLecture 1\nIntro to Computational Social Science\nFeb\nWed\n\n\nFeb 11\nLecture 2\nData 1: Gathering Data\nFeb\nWed\n\n\nFeb 18\nLecture 3\nData 2: Distributions in Empirical Data\nFeb\nWed\n\n\nFeb 25\nLecture 4\n\nFeb\nWed\n\n\nMar 3\nAssignment 1\nAssignment Due\nMar\nTue\n\n\nMar 4\nLecture 5\n\nMar\nWed\n\n\nMar 11\nLecture 6\n\nMar\nWed\n\n\nMar 18\nLecture 7\n\nMar\nWed\n\n\nMar 25\nLecture 8\n\nMar\nWed\n\n\nApr 1\nEaster Break\nEaster Break\nApr\nWed\n\n\nApr 7\nAssignment 2\nAssignment Due\nApr\nTue\n\n\nApr 8\nProject Assignment A\nIndependent Preparation Work\nApr\nWed\n\n\nApr 15\nProject Assignment A\nIndependent Preparation Work\nApr\nWed\n\n\nApr 21\nProject Assignment A\nAssignment Due\nApr\nTue\n\n\nApr 22\nLecture 9\nPresentations\nApr\nWed\n\n\nApr 29\nProject Assignment B\nIndependent Preparation Work\nApr\nWed\n\n\nMay 15\nProject Assignment B\nAssignment Due\nMay\nFri",
    "crumbs": [
      "Wiki",
      "Schedule"
    ]
  }
]